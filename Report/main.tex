\documentclass[10pt]{IEEEtran}
\pdfoutput=1

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage[table]{xcolor}
\usepackage{pdfpages}

\hypersetup{colorlinks=true,citecolor=[rgb]{0,0.4,0}}


\title{Analysis of the susceptibility of Twitter users}
\author{Søren Howe Gersager, Anders Lønberg Rahbek}

\begin{document}
\maketitle

\begin{abstract}
This report is the result of the final project in DTU course 02805: Social Graph and Interactions. The course subject was analyzing social networks using Twitter bots and the relation and interaction between bot and human. \\
This final project tries to look into what makes a human subsceptible to follow a Twitter bot.
\end{abstract}

\section{Introduction}
The purpose of DTU course 02805: Social Graph and Interactions is to make the students familiar with social networks and use network theory, natural language processing, datamining and machine learning to analyze these networks. All of the students were given the task of creating a Twitter bot with the purpose of "infiltrating" Twitter and gain followers. To gain followers the bot should try to imitate being a human user. As the followers and "Twitter presence" of the bot grew, it was used to try to influence the people of San Francisco through interventions. Interventions are created by letting all the bots tweet, retweet and favorite using a predetermined hashtag and in this way generate interactions from Twitter users. For the course, we created the bot JackBoHorseMan\cite{twitterprofile}, a 30-something male from Chicago, who loves animals, indie music, travelling and the Bulls.

\section{Implementation}
\subsection{Circadian Rhythm}
By using the job-scheduler cron we were able to make the bot interact with Twitter in a simulated circadian rhythm between 8:00 to 22:00 PST (San Francisco Timezone), with a random delay of 0 to 15 minutes included. This was done to make the bot appear more human.
The implementations in the next subsections all operate only in the circadian rhythm specified here.

\subsection{Tweeting}
The bot tweeted using two principles:
\begin{itemize}
\item Personal tweets: we created a list of predetermined tweets that the bot could select tweets from, communicating every-day situations as working late or waiting for the weekend or being tired on a monday that human users might relate or respond to.
\item Event tweets: The bot scraped the website of San Francisco Weekly and collected upcoming events like concerts, book readings, plays and tweeted about them in a fitting context randomly chosen from a list of fitting nouns and adjectives like "attending", "going to" and "rocking", "chill".
\end{itemize}
\subsection{Reciprocal Following}
Once a day we followed new twitter users based on two principles:
\begin{itemize}
\item Followback query: we performed a search for new users using a query on "followback", and followed them hoping they would follow us back. This principle was to artificially boost the number of followers and followed of the bot and thus look more attractive from a social point of view to potential human users.
\item Human query: we performed a search for new users based in San Francisco and followed them, the followers generated this way creates data for the later analysis of subsceptibility.\\
 If the users we followed didn't follow us back within 24 hours, we unfollowed them to make sure the ratio of followers to followed was kept as low as possible as a high ratio might arouse suspicion.
\end{itemize}
\subsection{Retweeting}
Once a day we retweeted content twice based on two different principles:
\begin{itemize}
\item Popularity: simply the most popular tweet based on number of retweets the bot had seen within the last 24 hours.
\item "Goodness": We trained a Naive Bayes classifier to be able to predict "good" tweets where the "goodness" criteria was that the tweet had 10 or more retweets. We used the following attributes: number of followers of the twitter account, age of the twitter account, number of links in the tweet, number of words in the tweet, and number of hashtags in the tweet. We divided the tweets into two data sets and used 2-fold cross validation on them. By doing this we could predict which tweets had the potential to be "good" retweeted tweets.
\end{itemize}

\subsection{Twitter Profile Variation}
The bot updated the Twitter profile every third day with a new profile banner based on a Google Image search, finding 50 on topics: horses, animals, indie, maldives, chicago bulls, san francisco monuments. This was done to make the bot appear more human as well as to pique the interest of potential followers.

\section{Interventions}
In the last part of the course, the whole class should make interventions through their bots. The intervention should hype some predifined hashtags through two personal tweets per day every Mondays, Wednesdays and Fridays plus thanksgiving. \\
Out strategy was to have the bot do it total autonomously so we didn't have to intervene. \\

We wrote all the personal tweets and their hashtags in a semicolon seperated text-file where the specified date was the first element - then the two tweets. Our bot could then read the file, make a dict with the dates as keys and the two tweets as values. \\

We had a window where we should tweet those two tweets - between 6pm and 9am danish time. To make some randomness to the time where the posts of the tweets would occur, we made a couple of random sleeps. One before the first tweet and two between the first and second tweet. \\
Two cron jobs ensured that the bot did every Mondays, Wednesdays and Fridays plus thanksgiving.

\section{Susceptibility analysis}
\subsection{User groups}
The foundation of our analysis consists of two groups: the susceptible and non-susceptible twitter users. The susceptible users group consists of 488 users, 487 of which allowed us to see their information and tweets. The non-susceptible users group consists of 1186 users, 1183 of which allowed us to see their information and tweets.

For performing analysis on tweets, 26 users from the non-susceptible group did not have any tweets, which shrunk the group down to 1160 when analysing tweets. Just one of the users in the susceptible group dit not have any tweets, which gives us a group size of 487 tweets.
We will analyze the following: Age of twitter account, number of followers, number of followed, number of tweets, average links of tweets, number of hashtags, tweets in comparison to retweets.
\\
NLP:
sentiment analysis, one group more positive or negative?
difference in readability measure
\\
Machine learning:
Training a classifier with above attributes: (naive bayes, logistic regression, decision tree)
\\
Networks:
In-out degree
clustering coefficient
average path length
degree distribution
which group follow the most high-profile users (100k followers or more).

\subsection{Statistics}
In comparing the data between the two groups, we will use a Welch t-test to test the null hypothesis that they have equal means or the alternative hypothesis that there is a difference in means of the two groups. We assume the populations to be normal-distributed with unequal variance.

\begin{table}[h]
\begin{tabular}{lllll}
\hline
Metric & Susceptible & Non-susceptible & t-value & p-value \\ \hline
Age & 1204.629 & 1456.636 & 6.290 & 4.952e-10 \\
Friends & 9638.393 & 10919.615 & -0.445 & 0.649 \\
Followers & 13369.934 & 61028.298 & -1.107 & 0.268 \\
Hashtags & 531.755 & 422.087 & -3.329 & 0.0009 \\
Statuses & 10627.834 & 11073.442 & -0.374 & 0.707 \\
Urls & 314.632 & 318.695 & 0.246 & 0.805
\end{tabular}
\caption{The table above shows the mean of each metric for the susceptible users and non-susceptible users, the hashtags metric are hashtags used for a maximum of 1000 tweets, the age of account is specified in days.

\end{table}
We choose a treshold of 5\% (0.05).
We find that for the age of account and the number of hashtags, the null hypothesis can be rejected with our threshold, and thus the alternative hypothesis must be accepted that the means are significantly different from each other.\\\\
\begin{figure}
	\caption{boxplot of hashtags}
	\centering
	\includegraphics[scale=0.5]{"figures/total_hashtags_boxplot"}
\end{figure}

\begin{figure}
	\caption{boxplot of age of accounts}
	\centering
	\includegraphics[scale=0.5]{"figures/age_of_account_boxplot"}
\end{figure}

\subsection{Theory}

\section{Conclusion}



\bibliographystyle{IEEEtran}
\bibliography{lyngby}
\begin{thebibliography}{9}

\bibitem{twitterprofile}
\url{twitter.com/JackBoHorseMan}
\textit{JackBoHorseMan Twitter Profile}

\end{thebibliography}

\clearpage
\onecolumn
\appendices
\section{Code listings}

\definecolor{darkgreen}{rgb}{0, 0.4, 0}
\lstset{language=Python,
  numbers=left,
  frame=bottomline,
  basicstyle=\scriptsize,
  identifierstyle=\color{blue},
  keywordstyle=\bfseries,
  commentstyle=\color{darkgreen},
  stringstyle=\color{red},
  literate={Ö}{{\"O}}1 {é}{{\'e}}1 {Å}{{\AA}}1,
}
\lstlistoflistings

\end{document}
