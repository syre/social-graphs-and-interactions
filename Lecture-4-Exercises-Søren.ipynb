{
 "metadata": {
  "name": "",
  "signature": "sha256:531f473a8fcc1da020796fa2055fc9786b4d5fea5c7c6bf6af1563cdc2124f7c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lecture 4 Exercises"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### NLPP1e"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####What is supervised classification? Explain in your own words.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Classification is the extraction of features from an input and classifying/grouping the input by one or more labels.\n",
      "\n",
      "Supervised classification is classification where the classifier has been trained or fed first using a resource where each input generates a feature set and the feature set plus a label generates a model.\n",
      "\n",
      "When training is completed the program can use the generated model to work on inputs not seen before and convert them to feature sets, which then can be converted to labels."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Work through the example on Gender Identification in 6.1, make it run in your own IPython Notebook - and make sure you know what's going on in the code.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "import nltk\n",
      "from nltk.corpus import names\n",
      "\n",
      "def gender_features(name):\n",
      "    return {\"last_letter\": name[-1]}\n",
      "\n",
      "\n",
      "names = ([(name, 'male') for name in names.words('male.txt')] +\n",
      "          [(name, 'female') for name in names.words('female.txt')])\n",
      "\n",
      "random.shuffle(names)\n",
      "\n",
      "featuresets = [(gender_features(n), g) for (n,g) in names]\n",
      "train_set, test_set = featuresets[500:], featuresets[:500]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "\n",
      "print(\"Guardians of the Galaxy cast:\")\n",
      "print(classifier.classify(gender_features(\"Star-Lord\"))) # Male\n",
      "print(classifier.classify(gender_features(\"Drax\"))) # Male\n",
      "print(classifier.classify(gender_features(\"Gamora\"))) # Female\n",
      "print(classifier.classify(gender_features(\"Rocket\"))) # Male\n",
      "print(classifier.classify(gender_features(\"Groot\"))) # Male\n",
      "print(classifier.classify(gender_features(\"Yondu\"))) # Male\n",
      "print(classifier.classify(gender_features(\"Nebula\"))) # Female\n",
      "\n",
      "print(nltk.classify.accuracy(classifier, test_set))\n",
      "print(classifier.show_most_informative_features(5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Guardians of the Galaxy cast:\n",
        "male\n",
        "male\n",
        "female\n",
        "male\n",
        "male\n",
        "male\n",
        "female\n",
        "0.762\n",
        "Most Informative Features\n",
        "             last_letter = u'k'             male : female =     41.9 : 1.0\n",
        "             last_letter = u'a'           female : male   =     34.2 : 1.0\n",
        "             last_letter = u'f'             male : female =     27.8 : 1.0\n",
        "             last_letter = u'p'             male : female =     11.9 : 1.0\n",
        "             last_letter = u'v'             male : female =     10.6 : 1.0\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Include solution to the \"your turn\" modification - and explain your choice of features. Do this before moving on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "import nltk\n",
      "from nltk.corpus import names\n",
      "\n",
      "def gender_features(name):\n",
      "    return {\"first_letter\": name[0], \"last_letter\": name[-1], \"length\": len(name), \"vowels\": len([n for n in name if n in \"aeiou\"])}\n",
      "\n",
      "\n",
      "names = ([(name, 'male') for name in names.words('male.txt')] +\n",
      "          [(name, 'female') for name in names.words('female.txt')])\n",
      "\n",
      "random.shuffle(names)\n",
      "\n",
      "featuresets = [(gender_features(n), g) for (n,g) in names]\n",
      "train_set, test_set = featuresets[500:], featuresets[:500]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "\n",
      "print(\"Guardians of the Galaxy cast:\")\n",
      "print(classifier.classify(gender_features(\"Star-Lord\"))) # Male\n",
      "print(classifier.classify(gender_features(\"Drax\"))) # Male\n",
      "print(classifier.classify(gender_features(\"Gamora\"))) # Female\n",
      "print(classifier.classify(gender_features(\"Rocket\"))) # Male\n",
      "print(classifier.classify(gender_features(\"Groot\"))) # Male\n",
      "print(classifier.classify(gender_features(\"Yondu\"))) # Male\n",
      "print(classifier.classify(gender_features(\"Nebula\"))) # Female\n",
      "print(nltk.classify.accuracy(classifier, test_set))\n",
      "print(classifier.show_most_informative_features(10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Guardians of the Galaxy cast:\n",
        "male\n",
        "male\n",
        "female\n",
        "male\n",
        "male\n",
        "male\n",
        "female\n",
        "0.742\n",
        "Most Informative Features\n",
        "             last_letter = u'a'           female : male   =     35.6 : 1.0\n",
        "             last_letter = u'k'             male : female =     31.3 : 1.0\n",
        "             last_letter = u'f'             male : female =     17.3 : 1.0\n",
        "             last_letter = u'p'             male : female =     12.6 : 1.0\n",
        "             last_letter = u'm'             male : female =      9.5 : 1.0\n",
        "             last_letter = u'd'             male : female =      9.3 : 1.0\n",
        "             last_letter = u'v'             male : female =      9.2 : 1.0\n",
        "             last_letter = u'o'             male : female =      8.2 : 1.0\n",
        "             last_letter = u'r'             male : female =      6.8 : 1.0\n",
        "             last_letter = u'w'             male : female =      6.2 : 1.0\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the last letter of a word was an indicator for gender, maybe the first letter would be aswell, and the length of the name and number of vowels."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Now, work through Choosing the Right Features.  What is overfitting? Why does the function gender_features2 overfit the training data? How does the notion of a development set help with overfitting?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Overfitting is modelling an algorithm too much for a specific data-set instead of making it general, it will then adapt the unusual quirks or features of the data-set and account for those.\n",
      "Training a model to be too \"perfect\" or \"specific\" for test-data, but which inadvertently makes it not good for general data-sets.\n",
      "\n",
      "gender_features2 overfit the training data, because it contains too many features which it apply to the training data.\n",
      "\n",
      "\n",
      "* Development set consisting of Training set and Dev-test set, subsets of the corpus\n",
      "* Test set, subset of the corpus\n",
      "\n",
      "The training set and dev-test set is for error analysis, we train a model using the training set and test it for errors using the dev-test set.\n",
      "We then adjust the features, and run the same test again to see if it has improved.\n",
      "\n",
      "This can then be repeated with a new Training set and Dev-test set to adjust our features.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Finally, work through Document Classification in 6.1 on your own. Let's say we label every tweet with more than 10 retweets as \"good\" and every tweet with less than that as \"bad\". Can we use this method to figure out what features makes some tweets popular? How?\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use supervised classification, and generate a corpus of tweets with either \"good\" or \"bad\" label and then find what features constitute the good. Or simply test our own tweets if it measures up to the good tweets before posting them."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Describe in your own words the evaluation metrics: Accuracy, Precision and recall, Confusion Matrices, and Cross-validation from chapter 6.3."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Accuracy** is the percentage of our classifier being right, so if our classifier is giving the correct gender to a name 80% of the time, that's the accuracy. As explained in the text a high accuracy in some cases is only valuable if the corpus is balanced, for instance when finding the correct word sense, as a corpus containing one sense of the word a lot of times would give an \"inflated\" accuracy\n",
      "\n",
      "Precision and recall are terms related to searching tasks, where finding relevant information is necessary:\n",
      "**Precision** is $TP/(TP+FP)$ where $TP$ is the number of True Positives, items identified correctly as *relevant*, and $FP$ is the number of False Positives, items *incorrectly* identified as relevant. Precision indicates how many items were actually relevant.\n",
      "\n",
      "**Recall** is $TP/(TP+FN)$ where $TP$ is the number of True Positives and $FN$ is the number of False Negatives, items incorrectly identified as *irrelevant*. Recall indicates how many relevant items were identified.\n",
      "\n",
      "**Confusion Matrices** is a way of visually presenting errors, a matrix of where cell[i,j] corresponds to how often i was predicted to be j, thus the diagonal of the matrix is correct predictions and all other cells are erronous predictions. The matrix allows one to see which are common cases of mispredictions.\n",
      "\n",
      "**Cross-validation** is a technique which implies that instead of just having one subset of the data be the training set and another subset of the data be a test set, we perform multiple evaluations on different test set and combine the scores. So we divide the corpus into N folds and train it with all the data minus one fold X and test it against the fold X, then we keep going until we have tested against all N folds. The combined folds we test against is all of our data and gives us and also allows us to see if our performance varies across different data sets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}